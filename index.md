--- 
layout: default 
---
<!--# About Me-->
I am currently working as post-doc at UT-Austin where I primarily work on [LDOS](https://ldos.utexas.edu/)
I graduated from UW-Madison, where I was Advised by [Dimitris Papailiopoulos](https://papail.io/) and [Shivaram Venkataraman](https://shivaram.org/) . 
My research interests are primarily in Systems for Machine Learning, especially around distributed training and inference of ML workloads. During my PhD I have been very fortunate to intern with Bilge Acun at FAIR, Amar Phanishayee at Microsoft Research and Yucheng Low at Apple.


During my time in Madison, when I was not being a grad student, I would be found racing keelboats on [Lake Mendota](https://www.mendotayc.org/racing) or alpine skiing in the winters. I also doubled up as a sailing instructor at the UW-Madison's Hoofers Sailing club. Since moving to Austin, I have been racing keelboats at Lake Travis. 

### Teaching
CS 395T, [Principles of Learned Systems](/cs395t)

#### Service
Reviewer: ICML '23, ICLR '23, Neurips '22, Neurips '21 <br />
ERC: MLSys '22, Usenix ATC '23


#### Publications 

* CHAI: Clustered Head Attention for Efficient LLM Inference. <br />
***S Agarwal***, *B Acun*, *B Hosmer*, *M Elhoushi*, *Y Lee*, *S Venkataraman*, *D Papailiopoulos*, *C Wu*. **ICML' 24** <br />
[[paper](https://arxiv.org/abs/2403.08058)]

* LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding  <br />
*M Elhoushi*, *A Shrivastava*, *D Liskovich*, *B Hosmer*, *B Wasti*, *L Lai*, *A Mahmoud*, *B Acun*, ***S Agarwal***, *A Roman*, *A Aly*, *B Chen*, *C Wu*. **ACL' 24** <br />
[[paper](https://arxiv.org/abs/2404.16710)]

* Decoding Speculative Decoding. <br />
*M Yan*, ***S Agarwal***, *S Venkataraman*.<br />
[[paper](https://arxiv.org/abs/2402.01528)]

* Blox: A Modular Toolkit for Deep Learning Schedulers.<br />
  ***S Agarwal***, *A Phanishayee*, *S Venkataraman*. **Eurosys'24**.<br />
[[paper](https://arxiv.org/abs/2312.12621)] [[source](https://github.com/msr-fiddle/blox)]

* Bagpipe: Accelerating deep recommendation model training.<br />
***S Agarwal***, *C Yan*, *Z Zhang*, *S Venkataraman*. **SOSP'23**.<br />
[[paper](https://dl.acm.org/doi/10.1145/3600006.3613142)][[source](https://github.com/uw-mad-dash/bagpipe)]

* Cuttlefish: Low-rank Model Training without All The Tuning. <br />
*H Wang*, ***S Agarwal***, *Y Tanaka*, *E Xing*, *D Papailiopoulos*. **MLSys'23**. <br /> 
[[paper](https://dl.acm.org/doi/10.1145/3600006.3613142)]

* Pufferfish: Communication-efficient models at no extra cost. <br />
*H Wang*, ***S Agarwal***, *D Papailiopoulos*. **MLSys'22**. <br />
[[paper](https://dl.acm.org/doi/10.1145/3600006.3613142)]

* On the utility of Gradient compression <br />
***S Agarwal***, *H Wang*, *S Venkataraman*, *D Papailiopoulos*. **MLSys'22**. <br />
[[paper](https://proceedings.mlsys.org/paper_files/paper/2022/file/773862fcc2e29f650d68960ba5bd1101-Paper.pdf)][[source](https://github.com/uw-mad-dash/GradCompressionUtility)]

* Adaptive Gradient Communication via Critical Learning Regime Identification. <br />
***S Agarwal***, *H Wang*, *K Lee*, *S Venkataraman*, *D Papailiopoulos*. **MLSys'21**. <br />
[[paper](https://arxiv.org/abs/2010.16248)][[source](https://github.com/uw-mad-dash/Accordion)]

* AutoFreeze: Automatically Freezing Model Blocks to Accelerate Fine-tuning. <br />
*Y Liu*, ***S Agarwal***, *S Venkataraman*. <br />
[[paper](https://arxiv.org/abs/2102.01386)]

* Attack of the tails: Yes, you really can backdoor federated learning. <br />
*H Wang*, *K Sreenivasan*, *S Rajput*, *H Vishwakarma*, ***S Agarwal***, *J Sohn*, *K Lee*, *D Papailiopoulos*.  **Neurips'21**  <br />
[[paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html)]
